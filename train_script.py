import argparse
import os
from pathlib import Path

import numpy as np
import torch
import torch.nn as nn
from torch.optim import Adam
from torch.optim.lr_scheduler import ReduceLROnPlateau
from torch.utils.data import DataLoader, WeightedRandomSampler
from torch.utils.tensorboard import SummaryWriter

from models.improved_cnn import ImprovedCNN
from utils.data_loading import remove_empty_folders, load_datasets
from utils.losses import FocalLoss
from utils.metrics import evaluate_with_f1, plot_confusion_matrix
from utils.training import EarlyStopping, train_one_epoch, evaluate, evaluate_per_class


# --- –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª ---
def main(args):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    writer = SummaryWriter(args.log_dir)

    # --- –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–∏—Å—Ç–µ–º–µ –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö ---
    print(f"\nüñ•Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
    if torch.cuda.is_available():
        print(f"üì¶ –í–∏–¥–µ–æ–∫–∞—Ä—Ç–∞: {torch.cuda.get_device_name(0)}")

    print("\nüîß –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—É—Å–∫–∞:")
    print(f"    üìÅ Train Dir:       {args.train_dir}")
    print(f"    üìÅ Val Dir:         {args.val_dir}")
    print(f"    üìÅ Check Dir:       {args.check_dir}")
    print(f"    üíæ Model Path:      {args.model_path}")
    print(f"    üì¶ Batch Size:      {args.batch_size}")
    print(f"    üîÅ Epochs:          {args.epochs}")
    print(f"    üöÄ Learning Rate:   {args.lr}")
    print(f"    ‚öñÔ∏è Weight Decay:    {args.weight_decay}")

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
    print("\nüß† –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏:")
    print(f"    üéØ Loss Function:   {'Focal Loss' if args.loss == 'focal' else 'CrossEntropy'}")
    if args.loss == 'focal':
        print(f"    üîç Focal Gamma:     {args.focal_gamma}")
    print(f"    üß© –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å CBAM: {'–î–∞' if args.use_cbam else '–ù–µ—Ç'}")
    if args.use_cbam:
        print(f"    üî¢ CBAM Ratio:      {args.cbam_ratio}")
        print(f"    üìè CBAM Kernel:     {args.cbam_kernel_size}")
    print(f"    üßä –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å BatchNorm: {'–î–∞' if args.use_batchnorm else '–ù–µ—Ç'}")

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏
    print("\nüõ°Ô∏è –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏:")
    print(f"    üß± –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å DropBlock: {'–î–∞' if args.use_dropblock else '–ù–µ—Ç'}")
    if args.use_dropblock:
        print(f"    üé≤ Drop Probability:  {args.drop_prob}")
        print(f"    üì¶ Block Size:        {args.block_size}")

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–π
    print("\nüé® –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–π:")
    print(f"    üß™ MixUp:           {'–í–∫–ª—é—á–µ–Ω' if args.use_mixup else '–û—Ç–∫–ª—é—á–µ–Ω'}")
    if args.use_mixup:
        print(f"    üé≤ MixUp Prob:      {args.mixup_prob}")
        print(f"    Œ± (alpha) MixUp:    {args.mixup_alpha}")
    print(f"    ü©π CutMix:          {'–í–∫–ª—é—á–µ–Ω' if args.use_cutmix else '–û—Ç–∫–ª—é—á–µ–Ω'}")
    if args.use_cutmix:
        print(f"    üé≤ CutMix Prob:      {args.cutmix_prob}")
        print(f"    Œ± (alpha) CutMix:   {args.cutmix_alpha}")

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
    print("\n‚öôÔ∏è –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è:")
    print(f"    ‚öôÔ∏è –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä:     {args.optimizer.upper()}")
    if args.optimizer == 'sgd':
        print(f"    üèÉ Momentum:        {args.momentum}")
    print(f"    üìâ Scheduler:       {args.scheduler.capitalize()}")
    print(f"    ‚è≥ Patience:         {args.patience}")
    print(f"    üèÅ Min LR:          {args.min_lr}")

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    print("\nüìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è:")
    print(f"    üìÇ Log Dir:         {args.log_dir}")
    print(f"    ‚ùå –°–æ—Ö—Ä–∞–Ω—è—Ç—å –æ—à–∏–±–∫–∏: {'–î–∞' if args.save_misclassified else '–ù–µ—Ç'}")
    if args.save_misclassified:
        print(f"    üìÅ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –æ—à–∏–±–æ–∫: {args.misclassified_dir}")
        print(f"    #Ô∏è‚É£ –ö–æ–ª-–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤:  {args.num_misclassified}")




    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    remove_empty_folders(args.train_dir)
    remove_empty_folders(args.val_dir)

    train_dataset, val_dataset, check_dataset = load_datasets(args.train_dir, args.val_dir, args.check_dir)
    num_classes = len(train_dataset.classes)

    class_counts = np.array(
        [len(os.listdir(os.path.join(args.train_dir, class_name))) for class_name in train_dataset.classes])
    class_weights = 1.0 / class_counts
    sample_weights = np.array([class_weights[train_dataset.targets[i]] for i in range(len(train_dataset))])
    sampler = WeightedRandomSampler(sample_weights, len(sample_weights))

    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, sampler=sampler)
    val_loader = DataLoader(val_dataset, batch_size=args.batch_size)
    check_loader = DataLoader(check_dataset, batch_size=args.batch_size)

    if args.balance_classes:
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –≤–µ—Å —Ä–µ–¥–∫–∏—Ö –∫–ª–∞—Å—Å–æ–≤
        class_weights = 1.0 / (class_counts + 1e-6)  # +1e6 —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –¥–µ–ª–µ–Ω–∏—è –Ω–∞ 0
        sample_weights = np.array([class_weights[y] for y in train_dataset.targets])
        sampler = WeightedRandomSampler(sample_weights, len(sample_weights))
        train_loader = DataLoader(train_dataset, batch_size=args.batch_size, sampler=sampler)
    else:
        train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)


    model = ImprovedCNN(
        num_classes=num_classes,
        use_cbam=args.use_cbam,
        cbam_ratio=args.cbam_ratio,
        cbam_kernel_size=args.cbam_kernel_size,
        use_dropblock=args.use_dropblock,
        drop_prob=args.drop_prob,
        block_size=args.block_size,
        use_batchnorm=args.use_batchnorm
    ).to(device)



    # --- –í—ã–±–æ—Ä —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å ---
    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)
    if args.loss == 'focal':
        # –£—Å–∏–ª–∏–≤–∞–µ–º —Ñ–æ–∫—É—Å –Ω–∞ –º–∞–ª—ã—Ö –∫–ª–∞—Å—Å–∞—Ö
        gamma = 2.0  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å 1.5 –¥–æ 2.0
        criterion = FocalLoss(alpha=class_weights_tensor, gamma=gamma)
    else:
        criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)

    optimizer = Adam(model.parameters(), lr=args.lr)
    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2)
    early_stopping = EarlyStopping(patience=5)

    best_accuracy = 0.0
    losses, accuracies = [], []

    for epoch in range(args.epochs):
        print(f"\n{'=' * 25} –≠–ø–æ—Ö–∞ {epoch + 1} / {args.epochs} {'=' * 25}")
        avg_loss = train_one_epoch(
            model,
            train_loader,
            criterion,
            optimizer,
            device,
            use_mixup=args.use_mixup,
            mixup_alpha=args.mixup_alpha,
            use_cutmix=args.use_cutmix,
            cutmix_prob=args.cutmix_prob,
            cutmix_alpha=args.cutmix_alpha
        )

        accuracy = evaluate(model, val_loader, device)
        losses.append(avg_loss)
        accuracies.append(accuracy)
        train_f1 = evaluate_with_f1(model, train_loader, device)
        val_f1 = evaluate_with_f1(model, val_loader, device)
        writer.add_scalar("Loss/train", avg_loss, epoch)
        writer.add_scalar("Accuracy/val", accuracy, epoch)
        writer.add_scalar("F1/train", train_f1, epoch)
        writer.add_scalar("F1/val", val_f1, epoch)



        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫ –≤ runs/matrix
        output_dir = Path("runs/matrix")
        output_dir.mkdir(parents=True, exist_ok=True)  # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏, –µ—Å–ª–∏ –æ–Ω–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç


        scheduler.step(accuracy)
        current_lr = optimizer.param_groups[0]['lr']
        writer.add_scalar("LR", current_lr, epoch)

        print(f"üìâ –°—Ä–µ–¥–Ω—è—è –ø–æ—Ç–µ—Ä—è (loss):        {avg_loss:.4f}")
        print(f"‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏:        {accuracy:.2f}%")
        print(f"üéØ F1-score –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏:        {val_f1:.4f}")
        print(f"üîÅ –¢–µ–∫—É—â–∏–π learning rate:        {current_lr:.6f}")

        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ EarlyStopping
        early_stopping(accuracy)  # –ü–µ—Ä–µ–¥–∞–µ–º —Ç–µ–∫—É—â—É—é —Ç–æ—á–Ω–æ—Å—Ç—å –≤ EarlyStopping

        if accuracy > best_accuracy:
            best_accuracy = accuracy
            torch.save(model, args.model_path)
            print("üíæ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞!")

        if early_stopping.early_stop:
            print("‚èπÔ∏è –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞.")
            break
        print("=" * 60)

    # –û—Ü–µ–Ω–∫–∞ –Ω–∞ check-—Å–µ—Ç–µ –ø–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—é –≤—Å–µ—Ö —ç–ø–æ—Ö
    check_accuracy = evaluate(model, check_loader, device)
    print(f"‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –ø—Ä–æ–≤–µ—Ä–æ—á–Ω–æ–º –Ω–∞–±–æ—Ä–µ (check_loader):        {check_accuracy:.2f}%")
    writer.add_scalar("Accuracy/check", check_accuracy, args.epochs)
    # --- –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –ø–æ –∫–ª–∞—Å—Å–∞–º ---
    conf_matrix, class_accuracies = evaluate_per_class(model, check_loader, device)
    plot_confusion_matrix(conf_matrix, check_dataset.classes, Path("runs/matrix") / "confusion_matrix_final.png")

    print(f"\nüìä –¢–æ—á–Ω–æ—Å—Ç—å –ø–æ –∫–ª–∞—Å—Å–∞–º (–Ω–∞ check-—Å–µ—Ç–µ):")
    for cls_name, acc in zip(check_dataset.classes, class_accuracies):
        print(f"    üì¶ {cls_name:25} ‚Äî {acc * 100:.2f}%")

    writer.close()


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Train ImprovedCNN with CBAM on E-commerce Product Images")

    # 1. –û—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–∞–Ω–Ω—ã—Ö –∏ –æ–±—É—á–µ–Ω–∏—è
    parser.add_argument('--train_dir', type=Path, default=Path("C:\\Users\\–î–º–∏—Ç—Ä–∏–π\\Desktop\\ECOMMERCE_PRODUCT_IMAGES\\train"),
                        help="–ü—É—Ç—å –∫ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É")
    parser.add_argument('--val_dir', type=Path, default=Path("C:\\Users\\–î–º–∏—Ç—Ä–∏–π\\Desktop\\ECOMMERCE_PRODUCT_IMAGES\\val"),
                        help="–ü—É—Ç—å –∫ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É")
    parser.add_argument('--check_dir', type=Path, default=Path("C:\\Users\\–î–º–∏—Ç—Ä–∏–π\\Desktop\\ECOMMERCE_PRODUCT_IMAGES\\check"),
                        help="–ü—É—Ç—å –∫ —Ç–µ—Å—Ç–æ–≤–æ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É")
    parser.add_argument('--model_path', type=str, default="best_model.pth",
                        help="–ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏")
    parser.add_argument('--batch_size', type=int, default=32,
                        help="–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞")
    parser.add_argument('--epochs', type=int, default=100,
                        help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è")
    parser.add_argument('--lr', type=float, default=0.001,
                        help="Learning rate")
    parser.add_argument('--weight_decay', type=float, default=0.001,
                        help="Weight decay –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞")

    # 2. –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
    parser.add_argument('--use_cbam', action='store_true', default=True,
                        help="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å CBAM –º–æ–¥—É–ª–∏")
    parser.add_argument('--cbam_ratio', type=int, default=16,
                        help="Ratio –¥–ª—è ChannelAttention –≤ CBAM")
    parser.add_argument('--cbam_kernel_size', type=int, default=7,
                        help="Kernel size –¥–ª—è SpatialAttention –≤ CBAM")

    # 3. –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏
    parser.add_argument('--use_dropblock', action='store_true', default=True,
                        help="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å DropBlock")
    parser.add_argument('--drop_prob', type=float, default=0.1,
                        help="–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥—Ä–æ–ø–∞ –¥–ª—è DropBlock")
    parser.add_argument('--block_size', type=int, default=5,
                        help="–†–∞–∑–º–µ—Ä –±–ª–æ–∫–∞ –¥–ª—è DropBlock")
    parser.add_argument('--use_batchnorm', action='store_true', default=True,
                        help="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å BatchNorm")

    # 4. –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–π
    parser.add_argument('--use_mixup', action='store_true', default=True,
                        help="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å MixUp –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—é")
    parser.add_argument('--mixup_prob', type=float, default=0.3,
                        help="–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è MixUp")
    parser.add_argument('--mixup_alpha', type=float, default=1.0,
                        help="Alpha –ø–∞—Ä–∞–º–µ—Ç—Ä –¥–ª—è MixUp")
    parser.add_argument('--use_cutmix', action='store_true', default=True,
                        help="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å CutMix –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—é")
    parser.add_argument('--cutmix_prob', type=float, default=0.3,
                        help="–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è CutMix")
    parser.add_argument('--cutmix_alpha', type=float, default=1.0,
                        help="Alpha –ø–∞—Ä–∞–º–µ—Ç—Ä –¥–ª—è CutMix")

    # 5. –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å
    parser.add_argument('--loss', type=str, choices=['ce', 'focal'], default='focal',
                        help="–¢–∏–ø —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å")
    parser.add_argument('--focal_gamma', type=float, default=2.0,
                        help="Gamma –ø–∞—Ä–∞–º–µ—Ç—Ä –¥–ª—è Focal Loss")

    # 6. –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
    parser.add_argument('--optimizer', type=str, choices=['adam', 'sgd'], default='adam',
                        help="–¢–∏–ø –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞")
    parser.add_argument('--momentum', type=float, default=0.9,
                        help="Momentum –¥–ª—è SGD")
    parser.add_argument('--scheduler', type=str, choices=['plateau', 'step', 'cosine'], default='plateau',
                        help="–¢–∏–ø —à–µ–¥—É–ª–µ—Ä–∞ –¥–ª—è LR")
    parser.add_argument('--patience', type=int, default=10,
                        help="Patience –¥–ª—è EarlyStopping –∏ ReduceLROnPlateau")
    parser.add_argument('--min_lr', type=float, default=0.0000000001,
                        help="–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π learning rate")
    # –í —Ä–∞–∑–¥–µ–ª–µ "–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è" –¥–æ–±–∞–≤—å—Ç–µ:
    parser.add_argument('--balance_classes', action='store_true', default=True,
                        help="–ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤ —á–µ—Ä–µ–∑ WeightedRandomSampler")


    # 7. –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    parser.add_argument('--log_dir', type=str, default="runs",
                        help="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –ª–æ–≥–æ–≤ TensorBoard")
    parser.add_argument('--save_misclassified', action='store_true', default=True,
                        help="–°–æ—Ö—Ä–∞–Ω—è—Ç—å –ø—Ä–∏–º–µ—Ä—ã —Å –æ—à–∏–±–∫–∞–º–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏")
    parser.add_argument('--misclassified_dir', type=str, default="misclassified",
                        help="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—à–∏–±–æ—á–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤")
    parser.add_argument('--num_misclassified', type=int, default=5,
                        help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º—ã—Ö –æ—à–∏–±–æ—á–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤")

    args = parser.parse_args()

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤
    if args.use_cutmix and args.use_mixup:
        print("–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –ò CutMix, –∏ MixUp –≤–∫–ª—é—á–µ–Ω—ã –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ. –≠—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –∏–∑–±—ã—Ç–æ—á–Ω–æ.")

    main(args)